## 랜덤 포레스트
- decision tree (나무 1개)
  - 여러개 심음
  - 샘플링
- 예측해야할 행의 갯수가 백만개
- 컴럼의 개수 200개 -> 100개
  - 나무 100개 심고 평균내기
  - 나무 1개당 컬럼을 10개로 하기
  - t1 mae : 20, t2 mae: 30, t3 mae :10 ...
    - t1-t100 mae : 20
    - feature importances
  - 샘플링 : 부트스트랩 샘플 (복원 추출)
  


```python
# 라이브러리 불러오기 
import numpy as np 
import pandas as pd 
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier

# 데이터 불러오기
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# input, target 분리 
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()


# 훈련데이터, 테스트 데이터 분리
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state = 42
)

# 모델링
rf = RandomForestClassifier(n_jobs=-1, random_state = 42)

# 모형 평가
scores = cross_validate(rf, train_input, train_target, return_train_score = True, n_jobs =-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 특성 중요
rf.fit(train_input, train_target)
# print(rf.feature_importances_)

# OOB 
rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)
rf.fit(train_input, train_target)
# print(rf.oob_score_)
```

    0.9973541965122431 0.8905151032797809
    




    RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=42)



## 그레이디언트 부스팅
- 경사 하강법의 원리를 이용함.
- T1 ~TN 증가하면서 오차를 보정해주고, 정확성을 높임
- 랜덤포레스트와의 차이
  - 랜덤포레스트는 각 나무간 상호연관성 없음
  - 부스팅은 각 나무간 상호 연관성 있음
- 단점
  - 속도가 너무 느림
- 대안
  - XGBoost, LightGBM


```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs= -1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
```

    0.8881086892152563 0.8720430147331015
    


```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=500, learning_rate =0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs= -1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
```

    0.9464595437171814 0.8780082549788999
    

- 특성 중요성


```python
# gb.fit(train_input, train_target)
print(gb.feature_importances_)
```

- 실무에서의 난이도
  - 비지도학습이 지도학습보다 어려움
  - 지도 : ai, 사람이 개입함
  - 비지도 학습 : 수치적으로 분류
- 주성분 분석
  - 이론적으로는 어려움
  - 좌표계 공간개념
    - 직교 + 회전
  - 공분산 등 (통계 관련 내용)
  - feature engineering 기법
  - standardscaler()
  - 현 ml의 문제점 컬럼의 갯수 매우 많음
- 차원 축소
  - 특성이 많으면 훈련데이터에 쉽게 과대 적합된다.
  - 특성을 줄여서 학습 모델의 성능을 향상시킴. 
  - 모델의 학습시간을 감소시켜줌.
  - 대표적인 방법론 하나가 pca,efa

- pca, efa의 차이
  - efa(탐색적 요인 분석), facter analysis
    - 예) 국어,수학, 과학, 영어
    - 예) 국어 40, 수학 100, 과학 100, 영어 30/ 귀 학생은 언어영역 수준은 낮은편이나, 수리 영역은 매우 수준이 높습니다.
 - 범주형 & 데이터셋에 많이 사용
-주성분 분석(pca)
   - 장비 1, 장비 2, 장비 3, 장비 4 ... 
   - pc1, pc2, pc3... pcn
   - 원래 가지고 있던 정보를 알 수 없음(정보손실)
   - 범주형 데이터 셋에는 사용 안됨
   - 무조건 수치형 데이터에서만 사용
   - pca 실행 전 반드시 표준화 처리(스케일링)
- p320
  - 


```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```

    --2022-07-05 04:53:22--  https://bit.ly/fruits_300_data
    Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11
    Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following]
    --2022-07-05 04:53:23--  https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy
    Resolving github.com (github.com)... 140.82.114.3
    Connecting to github.com (github.com)|140.82.114.3|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following]
    --2022-07-05 04:53:23--  https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 3000128 (2.9M) [application/octet-stream]
    Saving to: ‘fruits_300.npy’
    
    fruits_300.npy      100%[===================>]   2.86M  --.-KB/s    in 0.06s   
    
    2022-07-05 04:53:24 (47.0 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128]
    
    


```python
import numpy as np
fruits = np.load('/content/fruits_300.npy')
fruits_2d = fruits.reshape(-1,100*100)
 # 300개의 행, 10000개의 열
fruits_2d.shape
```




    (300, 10000)



- pca


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=50)
pca.fit(fruits_2d)
```




    PCA(n_components=50)




```python
print(pca.components_.shape)
```

    (50, 10000)
    


```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)    # n은 샘플 개수입니다
    # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. 
    rows = int(np.ceil(n/10))
    # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다.
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols, 
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()

```


```python
  draw_fruits(pca.components_.reshape(-1,100,100))
```


    
![png](/images/0705/output_14_0.png)
    



```python
# 머신러닝에서 컬럼의 개수를 10000개에서 50개로 줄임(수치 데이터에서만 적용 가능)
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)
```

    (300, 50)
    

- 훈련데이터, 테스트데이터 분리

## 설명된 분산
- 주 성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값


```python
# 92% 
# 원본이미지 압축
print(np.sum(pca.explained_variance_ratio_))
```

    0.9214928675263282
    


```python
plt.plot(pca.explained_variance_ratio_)
```




    [<matplotlib.lines.Line2D at 0x7f8dd3c68d90>]




    
![png](/images/0705/output_18_1.png)
    


---
title: "0704"
date: '2022-07-04'
---

## 확률적 경사 하강법 (단독적으로 사용되지 않음)
- 점진적 학습 (step, 보폭)
- 학습률
- xgboost,lightgbm, 딥러닝(이미지 분류, 자연어 처리, 옵티마이저)


### 샘플
- 신경망 이미지 데이터, 자연어
- 자율주행
- 한꺼번에 다 모델을 학습 어려움
  - 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함
- 결론적으로 확률적 경사 하강법


```python
### 손실함수
### 로지스틱 손실 함수
```


```python
import pandas as pd
fish = pd.read_csv("http://bit.ly/fish_csv_data")
fish.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159 entries, 0 to 158
    Data columns (total 6 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Species   159 non-null    object 
     1   Weight    159 non-null    float64
     2   Length    159 non-null    float64
     3   Diagonal  159 non-null    float64
     4   Height    159 non-null    float64
     5   Width     159 non-null    float64
    dtypes: float64(5), object(1)
    memory usage: 7.6+ KB
    

- 입력 데이터와 타깃데이터 분리


```python
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

fish_input.shape, fish_target.shape
```




    ((159, 5), (159,))



### 훈련세트와 테스트 데이터 분리


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    
    fish_input, fish_target, random_state = 42
)
```


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled=ss.transform(test_input)
```

### 모델링
 - 확률적 경사 하강법


```python
from sklearn.linear_model import SGDClassifier
```

### xgboost, lighgbm 코드
 - train-loss, train-accuary, test-loss, test-accu 


```python
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss = 'log', max_iter = 10, random_state=42)

sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled,test_target))
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    

- partal_fit()메서그 사용하면 추가학습


```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.8151260504201681
    0.85
    

### 에포크와 과대/ 과소 적합
 - 에포크 숫자가 적으면, 덜학습
 - early_stopping
   - 에포크 숫자를 1000, 손실 10, 9, 8, 3
   - 3에 도달한 시점이 150


```python
import numpy as np
sc = SGDClassifier(loss ='log', random_state= 42)
train_score = []
test_score = []

classes = np.unique(train_target)
# 300번 에포크 훈련 반복
# 훈련할때마다, train_score, test_score 추가
for _ in range(300):
  sc.partial_fit(train_scaled, train_target,classes=classes)
  train_score.append(sc.score(train_scaled,train_target))
  test_score.append(sc.score(test_scaled,test_target))
```


```python
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.legend(["train","test"])
plt.show()
```


    
![png](/images/0704/output_17_0.png)
    


## 결정트리
- wine 데이터 가져오


```python
import pandas as pd
wine = pd.read_csv('http://bit.ly/wine_csv_data')
print(wine.head())
wine.info()
wine.describe()
```

       alcohol  sugar    pH  class
    0      9.4    1.9  3.51    0.0
    1      9.8    2.6  3.20    0.0
    2      9.8    2.3  3.26    0.0
    3      9.8    1.9  3.16    0.0
    4      9.4    1.9  3.51    0.0
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 6497 entries, 0 to 6496
    Data columns (total 4 columns):
     #   Column   Non-Null Count  Dtype  
    ---  ------   --------------  -----  
     0   alcohol  6497 non-null   float64
     1   sugar    6497 non-null   float64
     2   pH       6497 non-null   float64
     3   class    6497 non-null   float64
    dtypes: float64(4)
    memory usage: 203.2 KB
    





  <div id="df-48528a69-cb29-4de8-87e8-3cfa8e7538c1">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alcohol</th>
      <th>sugar</th>
      <th>pH</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6497.000000</td>
      <td>6497.000000</td>
      <td>6497.000000</td>
      <td>6497.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>10.491801</td>
      <td>5.443235</td>
      <td>3.218501</td>
      <td>0.753886</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.192712</td>
      <td>4.757804</td>
      <td>0.160787</td>
      <td>0.430779</td>
    </tr>
    <tr>
      <th>min</th>
      <td>8.000000</td>
      <td>0.600000</td>
      <td>2.720000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>9.500000</td>
      <td>1.800000</td>
      <td>3.110000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>10.300000</td>
      <td>3.000000</td>
      <td>3.210000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>11.300000</td>
      <td>8.100000</td>
      <td>3.320000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>14.900000</td>
      <td>65.800000</td>
      <td>4.010000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-48528a69-cb29-4de8-87e8-3cfa8e7538c1')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-48528a69-cb29-4de8-87e8-3cfa8e7538c1 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-48528a69-cb29-4de8-87e8-3cfa8e7538c1');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




- 데이터 가공


```python
data = wine[['alcohol', 'sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
```


```python
target = wine['class'].to_numpy()
```

- 훈련데이터 분리



```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)

train_input.shape, test_input.shape, train_target.shape, test_target.shape
```




    ((5197, 3), (1300, 3), (5197,), (1300,))




```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.linear_model import LogisticRegression
lr= LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.7808350971714451
    0.7776923076923077
    

- 모델 만들기


```python
from pandas.core.common import random_state
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 8, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```

    0.89532422551472
    0.8569230769230769
    


    
![png](/images/0704/output_28_1.png)
    



```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```


    
![png](/images/0704/output_29_0.png)
    


### 노드란 무엇인가?
- 0 이면 레드와인
- 1 이면 화이트와인 : 4898


```python
train_target
```




    array([1., 0., 0., ..., 1., 1., 0.])




```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1,
          filled=True,
          feature_names=['alcohol', 'sugar','pH'])
plt.show()
# 루트노드 - 뿌리노드, 노드, 리트노드
```


    
![png](/images/0704/output_32_0.png)
    


- 불순도
  - 비율
  - 레드와인 5:5 화이트와인
  - 한 범주 안에서 서로 다른 데이터가 얼마나 섞여 있는지 나타냄
     - 흰색과 검은색이 각각 50개 섞여 있다
       - 불순도 최대 - 0.5
     - 흰색과 검은색이 완전 100% 분리
       - 흰색 노드 불순도 최소 - 0
       - 검은색 노드 불순도 최소 - 0
  - 엔드로피
    - 불확실한 정도를 의미 0-1사이
    - 흰색과 검은색이 각각 50개 섞여 있다
      - 엔트로피 최대 -1
   - 흰색과 검은색이 완전 100% 분리됨
     - 흰색 노드 엔트로피 최소 -0
     - 검은색 노드 엔트로피 최소 - 0

### 특성 중요도
 - 어떤 특성이 결정 트리 모델에 영향을 주었는가?


```python
print(dt.feature_importances_)

```

    [0.15533444 0.6675247  0.17714086]
    

## 현업에서 적용
 - 현업에서 decisiontreeclassifier(70년대)
 - 랜덤포레스트, xgboost 하이퍼파라미터 매우 많음
 - 


## 검증 세트
- 훈련세트와 테스트세트
- 훈련 : 교과서 공부하는 것, 훈련세트
모의평가
- 검증 : 강남대성 모의고사 문제집
- 테스트 : 6월 / 9월
- 실전 : 수능


```python
import pandas as pd
wine = pd.read_csv('http://bit.ly/wine_csv_data')
# print(wine.head())

data = wine[['alcohol', 'sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split
# 훈련 80% 테스트 20%
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)
train_input.shape, test_input.shape, train_target.shape, test_target.shape

```




    ((5197, 3), (1300, 3), (5197,), (1300,))




```python
# 훈련 80% 검증 20%
sub_input,val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size = 0.2, random_state=42
)
sub_input.shape,val_input.shape, sub_target.shape, val_target.shape
```




    ((4157, 3), (1040, 3), (4157,), (1040,))



- 훈련데이터 :train(x) sub_input, target이 훈련 데이터
- 검증데이터 - val_input, val_target
- 테스트데이터 - test_input,test_target



```python
# 모형 만들기
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print("훈련성과 : ", dt.score(sub_input, sub_target))
print("검증성과:", dt.score(val_input, val_target))
print("마지막 최종 :", dt.score(test_input,test_target))
```

    훈련성과 :  0.9971133028626413
    검증성과: 0.864423076923077
    마지막 최종 : 0.8569230769230769
    

- 훈련 : 87%
- 검증 : 86%
-------------
- 최종 : 55%

### 교차검증
- 각 데이터 셋을 반복 분할
- FOR LOOP
- 샘플링 편향적일 수 있음
- 교차 검증을 한다고 해서, 정확도가 무조건 올라가는 건 아님
  - 모형을 안정적으로 만들어 줘야함
    - 과대적합 방지


```python
import numpy as np
from sklearn.model_selection import KFold

df = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
# 데이터 K 폴드로 나눈다.
folds = KFold(n_splits = 5, shuffle = True)
for train_idx, valid_idx in folds.split(df):
  print(f'훈련 데이터 : {df[train_idx]}, 검증데이터: {df[valid_idx]}')
```

    훈련 데이터 : [ 1  2  3  4  5  7  8 10], 검증데이터: [6 9]
    훈련 데이터 : [ 1  3  5  6  7  8  9 10], 검증데이터: [2 4]
    훈련 데이터 : [ 2  3  4  6  7  8  9 10], 검증데이터: [1 5]
    훈련 데이터 : [ 1  2  3  4  5  6  9 10], 검증데이터: [7 8]
    훈련 데이터 : [1 2 4 5 6 7 8 9], 검증데이터: [ 3 10]
    

- 교차 검증 함수


```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
print("평균 : ", np.mean(scores['test_score']))
```

    {'fit_time': array([0.03095126, 0.0305624 , 0.03208995, 0.03173757, 0.01049256]), 'score_time': array([0.00121951, 0.01135445, 0.0013237 , 0.00117183, 0.00115395]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균 :  0.855300214703487
    


```python
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())
print(np.mean(scores['test_score']))
```

    0.855300214703487
    


```python
splitter = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)
scores = cross_validate(dt, train_input, train_target,cv = splitter)
print(np.mean(scores['test_score']))
```

    0.8574181117533719
    

## 하이퍼 파라미터 튜닝
- 그리드 서치
  - 사람이 수동적으로 입력
  - max_depth : [1,3,7]
- 랜덤 서치
  - 사람이 범위만 지정
  - max_depth: 1-10/ by random
- 베이지안 옵티마이제이션
- 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술을 AutoML이라 함
  - pycaret
- 각 모델마다 적게는 1-2개에서, 많게는 5-6 개의 매개변수를 제공한다.
  - xgboost 100개..?
- 하이퍼파라미터와 동시에 교차 검증을 수행

-교차검증 5번
교차검증 1번 돌때, max depth 3번 적용
 - max dept = 1,3,7
    -총 결과값 15번 3 x 5 나옴
 - criterion = gini, entropy


```python

```


```python
from sklearn.model_selection import GridSearchCV
params= {
    'criterion' : ['gini','entropy'],
    'max_depth': [1,3,7],
    'min_impurity_decrease':[0.0001,0.0002,0.0003,0.0004,0.0005]
}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input,train_target)
```




    GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
                 param_grid={'criterion': ['gini', 'entropy'],
                             'max_depth': [1, 3, 7],
                             'min_impurity_decrease': [0.0001, 0.0002, 0.0003,
                                                       0.0004, 0.0005]})




```python
print("best:" , gs.best_estimator_)
df=gs.best_estimator_
```

    best: DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005,
                           random_state=42)
    
